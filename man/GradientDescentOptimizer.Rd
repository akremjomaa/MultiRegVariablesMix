% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/GradientDescentOptimizer.R
\name{GradientDescentOptimizer}
\alias{GradientDescentOptimizer}
\title{Gradient Descent Optimizer}
\description{
Implements the standard Gradient Descent optimization algorithm.
The optimizer updates the weights by moving in the direction of the negative gradient of the loss function.
}
\section{Public fields}{
\if{html}{\out{<div class="r6-fields">}}
\describe{
\item{\code{learning_rate}}{Numeric. The learning rate for the optimizer.}
}
\if{html}{\out{</div>}}
}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-GradientDescentOptimizer-new}{\code{GradientDescentOptimizer$new()}}
\item \href{#method-GradientDescentOptimizer-update}{\code{GradientDescentOptimizer$update()}}
\item \href{#method-GradientDescentOptimizer-clone}{\code{GradientDescentOptimizer$clone()}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-GradientDescentOptimizer-new"></a>}}
\if{latex}{\out{\hypertarget{method-GradientDescentOptimizer-new}{}}}
\subsection{Method \code{new()}}{
Initialize the GradientDescentOptimizer with the given learning rate.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{GradientDescentOptimizer$new(learning_rate)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{learning_rate}}{Numeric. The learning rate for the optimizer.}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-GradientDescentOptimizer-update"></a>}}
\if{latex}{\out{\hypertarget{method-GradientDescentOptimizer-update}{}}}
\subsection{Method \code{update()}}{
Updates the weights using the Gradient Descent algorithm.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{GradientDescentOptimizer$update(W, gradient)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{W}}{Matrix. Current weight matrix.}

\item{\code{gradient}}{Matrix. Gradient of the loss function with respect to the weights.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
Updated weight matrix after applying the Gradient Descent update rule.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-GradientDescentOptimizer-clone"></a>}}
\if{latex}{\out{\hypertarget{method-GradientDescentOptimizer-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{GradientDescentOptimizer$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}
