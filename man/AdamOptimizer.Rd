% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/AdamOptimizer.R
\name{AdamOptimizer}
\alias{AdamOptimizer}
\title{Adam Optimizer}
\description{
Implements the Adam (Adaptive Moment Estimation) optimization algorithm.
This optimizer combines the benefits of momentum and RMSProp for efficient
and adaptive gradient-based optimization. It is widely used for training deep
neural networks and other machine learning models.
}
\section{Public fields}{
\if{html}{\out{<div class="r6-fields">}}
\describe{
\item{\code{learning_rate}}{Learning rate for the optimizer.}

\item{\code{beta1}}{Exponential decay rate for the first moment estimates.}

\item{\code{beta2}}{Exponential decay rate for the second moment estimates.}

\item{\code{epsilon}}{Small constant to avoid division by zero in the update rule.}

\item{\code{m}}{First moment estimate (initialized to zero).}

\item{\code{v}}{Second moment estimate (initialized to zero).}

\item{\code{t}}{Time step (initialized to zero).}
}
\if{html}{\out{</div>}}
}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-AdamOptimizer-new}{\code{AdamOptimizer$new()}}
\item \href{#method-AdamOptimizer-update}{\code{AdamOptimizer$update()}}
\item \href{#method-AdamOptimizer-clone}{\code{AdamOptimizer$clone()}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-AdamOptimizer-new"></a>}}
\if{latex}{\out{\hypertarget{method-AdamOptimizer-new}{}}}
\subsection{Method \code{new()}}{
Initializes the Adam optimizer with the given parameters.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{AdamOptimizer$new(
  learning_rate = 0.001,
  beta1 = 0.9,
  beta2 = 0.999,
  epsilon = 1e-08,
  weight_dim
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{learning_rate}}{The learning rate for weight updates. Default: 0.001.}

\item{\code{beta1}}{The exponential decay rate for the first moment estimates. Default: 0.9.}

\item{\code{beta2}}{The exponential decay rate for the second moment estimates. Default: 0.999.}

\item{\code{epsilon}}{A small constant for numerical stability. Default: 1e-8.}

\item{\code{weight_dim}}{Dimensions of the weight matrix (e.g., \code{c(rows, cols)}).}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-AdamOptimizer-update"></a>}}
\if{latex}{\out{\hypertarget{method-AdamOptimizer-update}{}}}
\subsection{Method \code{update()}}{
Updates the weights using the Adam optimization algorithm.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{AdamOptimizer$update(W, gradient)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{W}}{Current weight matrix.}

\item{\code{gradient}}{Gradient of the loss function with respect to the weights.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
Updated weight matrix after applying the Adam update.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-AdamOptimizer-clone"></a>}}
\if{latex}{\out{\hypertarget{method-AdamOptimizer-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{AdamOptimizer$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}
